"""
# Topline Historical Backfill

This script provides a way to backfill a summmary period from
historical data generated by the executive_reporting pipeline
[1]. This data is stored in two files representing the monthly and
weekly topline summaries. The reporting pipeline has been rewritten in
`telemetry-batch-view` as the ToplineSummaryView, which writes out
data to `telemetry-parquet/topline_summary/v1`.

Example Usage:

```
python -m mozetl.topline.historical_backfill \
    s3://path/to/v4_weekly.csv \
    weekly \
    net-mozaws-prod-us-west-2-pipeline-analysis \
    --prefix test-topline/topline_summary/v1
```

[1] https://git.io/v9mxF
"""
import logging

import click

from mozetl.topline.schema import historical_schema, topline_schema
from pyspark.sql import SparkSession, functions as F


logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def format_output_path(bucket, prefix):
    return "s3://{}/{}".format(bucket, prefix)


def backfill_topline_summary(historical_df, path):
    """ Backfill the topline summary with the historical dataframe.

    :historical_df dataframe: Data from the v4 historical data csv
    :path str: spark compatible path string to save partitioned parquet data
    """
    logging.info("Saving historical data to {}.".format(path))

    df = (
        historical_df
        .where(
              (F.col('geo') != 'all') &
              (F.col('os') != 'all') &
              (F.col('channel') != 'all'))
        .withColumn('report_start', F.date_format('date', 'yyyyMMdd'))
    )

    # Cast all elements from the csv file. Assume both schemas are flat.
    df = df.select(*[F.col(f.name).cast(f.dataType).alias(f.name)
                     for f in topline_schema.fields])

    # Use the same parititoning scheme as topline_summary
    df.write.partitionBy('report_start').parquet(path)


@click.command()
@click.argument('source_s3_path')
@click.argument('mode', type=click.Choice(['weekly', 'monthly']))
@click.argument('bucket')
@click.option('--prefix', default='topline_summary/v1')
def main(source_s3_path, mode, bucket, prefix):
    spark = (SparkSession
             .builder
             .appName('topline_historical_backfill')
             .getOrCreate())

    logging.info("Running historical backfill for {} executive report at {}."
                 .format(mode, source_s3_path))

    historical_df = spark.read.csv(source_s3_path,
                                   schema=historical_schema,
                                   header=True)
    output_path = format_output_path(bucket, '{}/mode={}'.format(prefix, mode))
    backfill_topline_summary(historical_df, output_path)

    spark.stop()
    logging.info("Finished historical backfill job.")


if __name__ == '__main__':
    main()
